---
title: "Business Case 1 - Longitudinal benchmarking"
output:
  rmdformats::downcute:
    lightbox: true
    gallery: true
    downcute_theme: "chaos"
pkgdown:
  as_is: true  
---

Along this vignette, the implementation of the longitudinal benchmarking used in Business Case 1 of the BIGG project is represented. This methodology basically consists on the statistical modelling of the general consumption of a building using weather and calendar features as inputs. Afterwards, the model is used to calculate energy usage indicators that are compared over the time.

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Load libraries

The 'biggr' package and other R packages are loaded to be used along this script. Besides, the configuration of the MLFlow infrastructure is set.

```{r error=F, warning=F, message=F}
library(biggr)
library(data.table)
library(ggplot2)
library(lubridate)
library(gridExtra)
library(plotly)
library(padr)
library(htmlwidgets)
library(carrier)
library(mlflow)
library(fs)
library(jsonlite)
library(tidyr)
mlflow_wd = as.character(path_home("PyVenvs/mlflow"))
Sys.setenv(MLFLOW_PYTHON_BIN=paste0(mlflow_wd,"/bin/python3"))
Sys.setenv(MLFLOW_TRACKING_URI="http://127.0.0.1:5001")
Sys.setenv(MLFLOW_VERBOSE=FALSE)
Sys.setenv(MLFLOW_BIN=paste0(mlflow_wd,"/bin/mlflow"))
settings <- fromJSON("settings_longitudinal_benchmarking.json")
#mlflow_client(mlflow_set_tracking_uri("http://127.0.0.1:5000"))
```

# Read the harmonised data

Read the following files:

* RDF file containing the general information of buildings, devices and their metadata
```
@prefix ns0: <https://bigg-project.eu/ontology#> .
@prefix ns1: <https://sws.geonames.org> .

<https://icaen.cat#BUILDING-00109> a ns0:Building ;
    ns0:buildingIDFromOrganization "00109" ;
    ns0:buildingName "Institut Eugeni d`Ors" ;
    ns0:buildingUseType "Others" ;
    ns0:hasCadastralInfos <https://icaen.cat#5276508DF3857E0001ES> ;
    ns0:hasLocationInfo <https://icaen.cat#LOCATION-00109> ;
    ns0:hasSpace <https://icaen.cat#BUILDINGSPACE-00109> .

<https://icaen.cat#5276508DF3857E0001ES> a ns0:CadastralInfo ;
    ns0:landArea "8723.0" ;
    ns0:landCadastralReference "5276508DF3857E0001ES" ;
    ns0:landType "Urban" .

<https://icaen.cat#AREA-GrossFloorArea-GPG-00109> a ns0:Area ;
    ns0:areaType "GrossFloorArea" ;
    ns0:areaUnitOfMeasurement "m2" ;
    ns0:areaValue "10279.0" .
```
* JSON file containing the time series of each metering device related with buildings.
```
{
  "941898a024310399210beb4d7f7bb890c568714cc006e7e6429cc6869ccb215c": [
    {
      "start": "2020-11-11T23:00:00Z",
      "end": "2020-11-12T00:00:00Z",
      "value": 0
    },
    {
      "start": "2020-11-12T00:00:00Z",
      "end": "2020-11-12T01:00:00Z",
      "value": 200
    },
    {
      "start": "2020-11-12T01:00:00Z",
      "end": "2020-11-12T02:00:00Z",
      "value": 245
    },
    {
      "start": "2020-11-12T02:00:00Z",
      "end": "2020-11-12T03:00:00Z",
      "value": 175
    }
  ],
  "123155674310399210beb4d7f7bb890c568714cc006e7e6429cc6869ccb215c": [
    {
      "start": "2020-11-11T23:00:00Z",
      "end": "2020-11-12T00:00:00Z",
      "value": 0.1
    },
    {
      "start": "2020-11-12T00:00:00Z",
      "end": "2020-11-12T01:00:00Z",
      "value": 12
    },
    {
      "start": "2020-11-12T01:00:00Z",
      "end": "2020-11-12T02:00:00Z",
      "value": 14.5
    },
    {
      "start": "2020-11-12T02:00:00Z",
      "end": "2020-11-12T03:00:00Z",
      "value": 81.2
    }
  ]
}
```

The 'biggr' library contains functionalities to automatically, join, resample, pad and align multiple time-series and building static data, considering the metadata condensed in the RDF files and the time-series data contained in the JSON file.

![BIGG-harmonised datasets in biggr library](images/harmonisation_biggr.png){width=70% height=70%}

```{r}
data_directory <- "../data/20_buildings_with_tariff"
if(!dir.exists(data_directory)){
  unzip(paste0(data_directory,".zip"),exdir = data_directory,junkpaths = T)
}
buildingsRdf <- suppressMessages(rdf_parse(
  list.files(data_directory,".ttl",full.names = T)[1], format = "turtle"))
timeseriesObject <- unlist(lapply(list.files(data_directory,"json",full.names=T),
         function(x){jsonlite::fromJSON(x)}),recursive=F)
get_all_buildings_list(buildingsRdf)
```


```{r}
buildingSubject <- "https://icaen.cat#BUILDING-01438"
buildingId <- get_building_identifiers(buildingSubject)
energyType <- "electricity"
deviceAggregator <- "totalElectricityConsumption"
modelName <- "HourlyDynamicModel"
buildingData <- get_device_aggregators_by_building(
  buildingsRdf, timeseriesObject, 
  allowedBuildingSubjects =buildingSubject, 
  allowedDeviceAggregators=c("totalElectricityConsumption","outdoorTemperature"), 
  useEstimatedValues=F, ratioCorrection=T)
tz <- get_tz_building(buildingsRdf, buildingSubject)
```

## Get the data for a specific building

Select a specific buildingId and build the dataset used in the analytics process.

```{r}
df <- data.frame(
  "time" = buildingData[[buildingSubject]]$df$time,
  "temperature" = buildingData[[buildingSubject]]$df[,paste0("outdoorTemperature",".AVG_Temperature")],
  "Qe" = buildingData[[buildingSubject]]$df[,
    paste0(deviceAggregator,".SUM_",
           buildingData[[buildingSubject]]$metadata$measuredProperty[
             buildingData[[buildingSubject]]$metadata$deviceAggregatorName==deviceAggregator
           ][1])],
  "Qe_cost" = buildingData[[buildingSubject]]$df[,
    paste0(deviceAggregator,".SUM_EnergyCost")],
  "Qe_emissions" = buildingData[[buildingSubject]]$df[,
    paste0(deviceAggregator,".SUM_EnergyEmissions")],
  "Qe_price" = buildingData[[buildingSubject]]$df[,
    paste0(deviceAggregator,".AVG_",
          buildingData[[buildingSubject]]$metadata$measuredProperty[
            buildingData[[buildingSubject]]$metadata$deviceAggregatorName==deviceAggregator
          ][1],"_EnergyPrice")],
  "Qe_emissionsFactor" = buildingData[[buildingSubject]]$df[,
    paste0(deviceAggregator,".AVG_",
          buildingData[[buildingSubject]]$metadata$measuredProperty[
            buildingData[[buildingSubject]]$metadata$deviceAggregatorName==deviceAggregator
          ][1],"_EnergyEmissionsFactor")]
  )
df <- df[min(which(is.finite(df$Qe))):(nrow(df)-min(which(is.finite(rev(df$Qe))))),]
```

```{r}
datePeriod <- as.duration(as.Date(max(df$time[is.finite(df$Qe)]), tz=tz) - 
                                   as.Date(min(df$time[is.finite(df$Qe)]), tz=tz))
```

Add the calendar features to the dataset using the timezone contained in the RDF for that building.

````{r}
####
# Calendar features and filtering of holidays and special periods ----
####
    
# Detect the COVID influenced period if data is over COVID lockdown & return-to-normality period
# with a minimum of one-year period.
    
  if( as.Date(quantile(df$time[is.finite(df$Qe)],0.8), tz=tz) > (
        ( as.Date(min(df$time[is.finite(df$Qe)]), tz=tz) +
            months(settings$DataCleaning$CheckForDisruption$minInitialMonths)
        )
      ) ||
      ( as.Date(quantile(df$time[is.finite(df$Qe)],0.8), tz=tz) >= 
        as.Date(settings$DataCleaning$CheckForDisruption$minIniDate)
      ) ||
      ( ( as.Date(min(df$time[is.finite(df$Qe)]), tz=tz) +
          months(settings$DataCleaning$CheckForDisruption$minInitialMonths) 
        ) <= as.Date(settings$DataCleaning$CheckForDisruption$maxEndDate) 
      )
  ) {
    
    write("* Detecting the time period with affectance by SARS-CoV2 lockdowns",stderr())
    
    minIniDate = max(min(as.Date(df$time[is.finite(df$Qe)], tz=tz)) +
                       months(settings$DataCleaning$CheckForDisruption$minInitialMonths) ,
                     as.Date(settings$DataCleaning$CheckForDisruption$minIniDate))
    maxIniDate = max(min(as.Date(df$time[is.finite(df$Qe)], tz=tz)) +
                       months(settings$DataCleaning$CheckForDisruption$minInitialMonths) ,
                     as.Date(settings$DataCleaning$CheckForDisruption$maxIniDate))
    minEndDate = max(maxIniDate,
                     min(as.Date(quantile(df$time[is.finite(df$Qe)],0.8), tz=tz),
                         as.Date(settings$DataCleaning$CheckForDisruption$minEndDate)))
    maxEndDate = max(minEndDate,
                     min(as.Date(quantile(df$time[is.finite(df$Qe)],0.8), tz=tz),
                         as.Date(settings$DataCleaning$CheckForDisruption$maxEndDate)))
    covid_affected_period <- suppressWarnings(detect_disruptive_period(
      data=df, consumptionColumn="Qe",
      temperatureColumn="temperature", timeColumn = "time", tz=tz,
      minIniDate = minIniDate, maxIniDate = maxIniDate,
      minEndDate = minEndDate, maxEndDate = maxEndDate, 
      checkFor=settings$DataCleaning$CheckForDisruption$checkFor,
      minDecrementPercentualAffectation = settings$DataCleaning$CheckForDisruption$minDecrementPercentualAffectation,
      minIncrementPercentualAffectation = settings$DataCleaning$CheckForDisruption$minIncrementPercentualAffectation))
    
    covid_affected_period <- tryCatch(
      seq.Date(covid_affected_period$minDate,covid_affected_period$maxDate,by = "days"),
      error = function(e)NULL)
  } else {
    covid_affected_period <- c()
  }
  
  # Detect holidays
  write("* Detecting the holidays",stderr())
  holidaysDates <- detect_holidays_in_tertiary_buildings(
    data = df, 
    consumptionColumn = "Qe", 
    timeColumn = "time",
    tz=tz,
    ignoreDates = covid_affected_period,
    plotDensity = F)
  
  # Add the calendar components
  df <- df %>% calendar_components(
    localTimeZone = tz,
    holidays = holidaysDates,
    inplace=T
  )
  
  h <- ggplot(df[df$time>min(df$time[is.finite(df$Qe)],na.rm=T),]) + geom_line(aes(time,Qe)) +
    geom_point(data=df[df$isHolidays==T,],mapping = aes(time,Qe),col="red")
  saveWidget(ggplotly(h),"holidays.html", 
             selfcontained = T)
  ggplotly(h)
```

## Plot the data

In the following chunk, the total electricity consumption and the outdoor temperature signals are plotted over time.
```{r}
ts_p <- ggplot(
        reshape2::melt( df %>% select(time, Qe, temperature) %>% suppressMessages(pad()), 
                        "time")
      ) + 
        geom_line(
          aes(time,value)
        ) + 
        facet_wrap(~variable, scales = "free_y", ncol=1) +
        theme_bw()
saveWidget(ggplotly(ts_p),"raw_consumption_temperature.html",
           selfcontained = T)
```

Then, with the objective of understanding the relation between the electricity consumption and the outdoor temperature, a scatter plot of the electricity consumption and outdoor temperature is depicted.

```{r}
grid.arrange(
  ggplot(df[,c("temperature","Qe")]) +
      geom_point(
        aes(temperature, Qe),
        size=0.05
      )
)
```

Additionally, the complete set of daily / weekly load curves are plotted to visually check the existance of difference energy usage profiles.

```{r}
# Daily profiles
ggplot(df) + 
  geom_line(
    aes(hour, Qe, group=date),
    alpha=0.05
  ) + xlab("hour of the day")
```

```{r}
# Weekly profiles
ggplot(df) + 
  geom_line(
    aes(weekhour, Qe, group = paste(strftime(localtime,"%Y"),
                                    strftime(localtime,"%U"))),
    alpha=0.1
  ) + xlab("hour of the week")
```

# Filter the consumption outliers

In order to improve the modelling performance, the days with outliers are not considered. The methodology used to detect them is a calendar-based quantile regression model.

```{r}
####
# Outliers detection ----
####

write("* Detecting the outliers",stderr())
if(all(c("value","window") %in% colnames(df)))
  df <- df %>% select(-value, -window)
df <- 
  df %>%
  select(!(contains("outliers") | contains("upperPredCalendarModel") | 
             contains("lowerPredCalendarModel"))) %>%
  left_join(
    detect_ts_calendar_model_outliers(
      data = .,
      localTimeColumn = "localtime",
      valueColumn = "Qe", 
      calendarFeatures = settings$DataCleaning$OutliersDetection$calendarFeatures$PT1H,
      mode = settings$DataCleaning$OutliersDetection$mode,
      upperModelPercentile = settings$DataCleaning$OutliersDetection$upperModelPercentile,
      lowerModelPercentile = settings$DataCleaning$OutliersDetection$lowerModelPercentile,
      upperPercentualThreshold = settings$DataCleaning$OutliersDetection$upperPercentualThreshold,
      lowerPercentualThreshold = settings$DataCleaning$OutliersDetection$lowerPercentualThreshold,
      window = settings$DataCleaning$OutliersDetection$window$PT1H,
      outputPredictors = T,
      holidaysCalendar = holidaysDates,
      daysThatAreOutliers = covid_affected_period,
      logValueColumn = T,
      autoDetectProfiled = T),
    by = "localtime"
  )
abnormalDays <- sort(df %>% 
  group_by(date) %>% summarise(out=sum(outliers)) %>%
  filter(out>0) %>% select(date) %>% unlist(.) %>% as.Date(.))
df$abnormalDay <- df$date %in% abnormalDays

g <- ggplot(df[,c("localtime","Qe","outliers",
                  "upperPredCalendarModel","lowerPredCalendarModel")]) +
  geom_line(aes(localtime,Qe)) +
  geom_ribbon(aes(localtime,ymax=upperPredCalendarModel,ymin=lowerPredCalendarModel),
              col="blue",alpha=0.5)
if(!all(is.na(df$outliers) | df$outliers==F)) 
  g <- g + geom_point(aes(localtime,ifelse(outliers,Qe,NA)),col="yellow")
saveWidget(ggplotly(g), "outliers_plot.html", selfcontained = T)
```

# Clustering and classification of the daily load curves

In order to estimate, and consider during the data modelling, different patterns in daily load curves, such as weekend / weekdays patterns or holidays, a clustering is performed. This process will detect the most common daily load curves and classifies each day to each of the categories detected. How it classifies? Normally, it assigns the cluster with the lowest euclidian distances, but in the case of consumption measures with outliers, a model based on calendar features is used. Therefore, the classification algorithm will predict the daily consumption pattern that would have been followed if no anomalies where detected (e.g. Covid lockdown period).

```{r}
####
# Clustering and classification of daily load curves ----
####

write("* Detecting the most common daily load curves",stderr())
if(is.null(settings$DailyLoadCurveClustering$maxTrainingMonths)){
  maxDateForClustering <- NULL
} else {
  ht <- hourly_timesteps(720*settings$DailyLoadCurveClustering$maxTrainingMonths,detect_time_step(df$time))
  maxDateForClustering <- sort(df$date[df$outliers==F & !is.na(df$outliers)],)[
    if(ht > length(df$date[df$outliers==F & !is.na(df$outliers)])){
      length(df$date[df$outliers==F & !is.na(df$outliers)])
    } else {ht}
  ]
}
clust <- clustering_dlc(
  data = df,
  consumptionFeature = "Qe", 
  outdoorTemperatureFeature = "temperature", 
  localTimeZone = tz,
  kMax = settings$DailyLoadCurveClustering$kMax, 
  kMin = settings$DailyLoadCurveClustering$kMin,
  inputVars = settings$DailyLoadCurveClustering$inputVars, 
  loadCurveTransformation = settings$DailyLoadCurveClustering$loadCurveTransformation,
  balanceOutdoorTemperatures = settings$DailyLoadCurveClustering$balanceOutdoorTemperatures,
  ignoreDates =
    df %>% group_by(date) %>% summarise(outliers=sum(outliers)>0) %>% filter(
      if(is.null(maxDateForClustering)){
        outliers==T
      } else {
        outliers==T | (date >= maxDateForClustering)
      }) %>% select(date) %>% 
    unlist %>% as.Date,
  holidaysDates = holidaysDates,
  nDayParts = settings$DailyLoadCurveClustering$nDayParts,
  normalisationMethod = "range01"
)

if("s" %in% colnames(df))
  df <- df %>% select(-s)
df <- df %>% left_join(clust$dailyClassification, by="date")

classif <- classification_dlc(
  data = df[is.na(df$s),], 
  consumptionFeature = "Qe",
  outdoorTemperatureFeature = "temperature", 
  localTimeZone = tz,
  holidaysDatesFeature = "holidaysDate",
  abnormalDaysFeature = "abnormalDay",
  clustering = clust,
  methodNormalDays = "clusteringCentroids",
  methodAbnormalDays = "classificationModel"
)
df <- df %>% left_join(classif$dailyClassification,by="date")
df$s <- ifelse(!is.na(df$s.x),df$s.x,df$s.y)
df$s_origin <- ifelse(!is.na(df$s.x),"clustering","classification")
df$s.x <- NULL
df$s.y <- NULL
```

```{r}
p_sig <- ggplot(df[df$s_origin=="clustering",] %>%
                group_by(date) %>%
                summarise(Qe=sum(Qe),
                          temperature=mean(temperature,na.rm=T),
                          s=first(s))) +
    geom_point(aes(temperature,Qe,col=s)) +
    xlab(bquote("temperature (ºC)")) + 
    ylab("consumption (kWh)")+ theme_bw()
saveWidget(ggplotly(p_sig),"clustering_signature.html", 
             selfcontained = T)
p <- ggplot(df) +
    geom_line(
      aes(hour, Qe, group=date, col=s_origin),
      alpha=0.1
    ) +
    xlab("hour of the day") +
    facet_wrap(~s) +
    theme_bw()
saveWidget(ggplotly(p),"clustering_dlc.html", 
             selfcontained = T)
p_ts <- ggplot(df %>% group_by(date) %>% 
                 summarise("s"=first(s),
                           "Qe"=sum(Qe),
                           "s_origin"=first(s_origin))) + 
  geom_point(
    aes(date, Qe, col=s, shape=s_origin),
    alpha=0.7
  ) + 
  xlab("time") +
  theme_bw()
saveWidget(ggplotly(p_ts),"clustering_dlc_ts.html", 
             selfcontained = T)
```


# Initialize an MLFlow experiment

Start or continue an MLFlow experiment using the buildingId as the name of the experiment 

```{r}
identifier <- paste(buildingId,deviceAggregator,sep='~')
experimentId <- tryCatch(
    mlflow_create_experiment(identifier, artifact_location = paste0(settings$MLFlow$WorkingDirectory,"/mlruns")),
    error = function(e){
      experiment <- mlflow_get_experiment(name=identifier)
      if (experiment$lifecycle_stage!="active") mlflow_restore_experiment(experiment$experiment_id)
      experiment$experiment_id
    }
  )
```

# Configuration of the data transformation processes

Firstly, estimate the change point temperature of the building and check if weather dependence during heating or cooling period exists.

```{r}
###
# Check weather dependence by group of daily load curves ----
###
write("* Detecting if exists any weather dependence in energy consumption",stderr())

pdf("clustering_dlc_wdep.pdf",4,3)
weatherDependenceByCluster <- suppressWarnings(
  get_change_point_temperature_v2(
  consumptionData = df[ df$s_origin=="clustering",c("time","Qe","s")],
  weatherData =  df[ df$s_origin=="clustering",c("time","temperature")],
  consumptionFeature = "Qe",
  temperatureFeature = "temperature",
  consumptionGroupFeature = "s",
  localTimeZone = tz,
  plot=T
))
dev.off()

wdep <- as.list(setNames(matrixStats::colMeans2(
  apply(as.data.frame(weatherDependenceByCluster) %>% select(-s),1:2,as.numeric),
  na.rm = T),c("tbalh","tbalc","heating","cooling")))
df$wdeph <- factor(df$s, levels=sort(unique(df$s)))
levels(df$wdeph) <- weatherDependenceByCluster$heating[order(levels(df$wdeph))]
df$wdeph <- as.numeric(as.character(df$wdeph))
df$wdepc <- factor(df$s, levels=sort(unique(df$s)))
levels(df$wdepc) <- weatherDependenceByCluster$cooling[order(levels(df$wdepc))]
df$wdepc <- as.numeric(as.character(df$wdepc))
wdep$heating <- wdep$heating > 0
wdep$cooling <- wdep$cooling > 0
```

Then, set initial values for the data transformation parameters.

```{r}
###
# Setting the model parameters and transformations to be done during the model training ----
###

write("* Setting model parameters and transformations",stderr())
generalParams <- list(
  "nhar"=list(
    "datatype"="integer",
    "nlevels"=2,
    "min"=8,
    "max"=10
  ),
  "maxh"=list(
    "datatype"="float",
    "nlevels"=3,
    "min"=10,
    "max"=40
  ),
  "maxc"=list(
    "datatype"="float",
    "nlevels"=3,
    "min"=10,
    "max"=40
  ),
  "alpha"=list(
    "datatype"="discrete",
    "levels"=c(0.6,0.75,0.85,0.9,0.95,0.975,0.985,0.99,0.995)
  ),
  "lambda"=list(
    "datatype"="discrete",
    "levels"=c(
      get_lpf_smoothing_time_scale(data.frame("time"=df$time),
        if(wdep$heating && wdep$cooling){
         6*31*24
        } else if (wdep$heating || wdep$cooling) {
         4*31*24
        } else {
         2*31*24
        })
    )
  )
)
```

Afterwards, the data transformation operations that need to be done over the initial dataset, either in training or prediction mode, are described. This operations can contain parameters specified in general_params, always refering to them as param\$<variable>, e.g. param\$nhar.

```{r}
    generalTransformationSentences <- list(
      # Classify the daily load curves in case it was not predicted
      "s" = "
        if(exists('s')){
          factor(
            s,
            levels=rownames(clusteringResults$absoluteLoadCurvesCentroids),
            labels=rownames(clusteringResults$absoluteLoadCurvesCentroids)
          )
        } else {
          factor(classification_dlc(
            ..., 
            consumptionFeature = 'Qe',
            outdoorTemperatureFeature = 'temperature', 
            localTimeZone = tz,
            holidaysDatesFeature = 'holidaysDate',
            abnormalDaysFeature = 'abnormalDay',
            clustering = clusteringResults,
            methodNormalDays = 'clusteringCentroids',
            methodAbnormalDays = 'classificationModel'
          )$sRaw,
          levels=rownames(clusteringResults$absoluteLoadCurvesCentroids),
          labels=rownames(clusteringResults$absoluteLoadCurvesCentroids))
        }",
      
      "intercept_s" = c("s"),
      
      # Avoid errors when there is no holidays detected
      "isHolidays" = "
        if(length(unique(as.character(isHolidays)))==1){
          rep(1,isHolidays)
        } else {
          isHolidays
        }",
      
      # Fourier series components of the hour of the day by weekdays and weekends. 
      "daily_seasonality" = c(
        "fs_components(...,featuresName='hour',nHarmonics=param$nhar,inplace=F)"
        ,"weekday"
      ),
      
      # Fourier series components of the hour of the day by weekdays and weekends. 
      "weekly_seasonality" = c(
        "fs_components(...,featuresName='weekdayNum',nHarmonics=2,inplace=F)"#,
        #"s"
      ),
      
      # Fourier series components of the hour of the day by weekdays and weekends. 
      "yearly_seasonality" = c(
        "fs_components(...,featuresName='monthInt',nHarmonics=2,inplace=F)"#,
        #"s"
      ),
      
      # Fill some gaps in the outdoor temperature time series.
      "temperature" = "na.locf(
                          na.locf(
                            na.approx(temperature,na.rm = F),
                            fromLast = T,na.rm = T
                          ),
                          na.rm=T)",
      
      # Low Pass Filtered (LPF) outdoor temperature
      "tlpf" = "lpf_ts(...,featuresNames='temperature',smoothingTimeScaleParameter=param$alpha,
                       inplace=F)",
      
      # Depending the cluster and the weather dependence analysis, define activations of the HVAC system
      "onOffHeating" = "as.numeric(as.character(
                          factor(s,levels=weatherDependenceByCluster$s,
                                 labels = weatherDependenceByCluster$heating)
                        ))",
      "onOffCooling" = "as.numeric(as.character(
                          factor(s,levels=weatherDependenceByCluster$s,
                                 labels = weatherDependenceByCluster$cooling)
                        ))",
      "temperatureBalanceHeating" = "as.numeric(as.character(
                          factor(s,levels=weatherDependenceByCluster$s,
                                 labels = ifelse(is.finite(weatherDependenceByCluster$tbalh),
                                                 weatherDependenceByCluster$tbalh,18))
                          ))",
      "temperatureBalanceCooling" = "as.numeric(as.character(
                          factor(s,levels=weatherDependenceByCluster$s,
                                 labels = ifelse(is.finite(weatherDependenceByCluster$tbalc),
                                                 weatherDependenceByCluster$tbalc,24))
                          ))",
      
      # Estimate the heating degrees based on a heating balance temperature 
      # and the LPF temperature series
      "heatingLpf" = "degree_raw(...,featuresName='tlpf',
                        baseTemperature=NULL,
                        baseTemperatureName='temperatureBalanceHeating',
                        mode='heating',outputFeaturesName='heatingLpf',maxValue=param$maxh,
                        hysteresisBaseTemperature = 0,
                        inplace=F)",
      
      # Estimate the cooling degrees based on a cooling balance temperature 
      # and the LPF temperature series
      "coolingLpf" = "degree_raw(...,featuresName='tlpf',
                        baseTemperature=NULL,
                        baseTemperatureName='temperatureBalanceCooling',
                        mode='cooling',outputFeaturesName='coolingLpf',maxValue=param$maxc,
                        hysteresisBaseTemperature = 0,
                        inplace=F)",
      
      # Squared versions of the heating and cooling degrees
      "heatingLpf2" = "heatingLpf^2",
      "coolingLpf2" = "coolingLpf^2",
      
      # Check if exists heating or cooling degrees at every timestep 
      "heatingLpfBool" = "ifelse(heatingLpf>0,1,0)",
      "coolingLpfBool" = "ifelse(coolingLpf>0,1,0)",
      
      # Regression components for the heating degrees depending on the hour of the day 
      #and weekday/weekend
      "th"=c("heatingLpf",
             "fs_components(...,featuresName='hour',
                            nHarmonics=ceiling(param$nhar/2),inplace=F)",
             "s"),
      
      # Regression components for the cooling degrees depending on the hour of the day 
      # and weekday/weekend
      "tc"=c("coolingLpf",
             "fs_components(...,featuresName='hour',
                            nHarmonics=ceiling(param$nhar/2),inplace=F)",
             
             "s"),
      
      # Regression components for the heating degrees depending on the hour of the day 
      #and weekday/weekend
      "th2"=c("heatingLpf2",
              "fs_components(...,featuresName='hour',
                             nHarmonics=ceiling(param$nhar/2),inplace=F)",
              "s"),
      
      # Regression components for the cooling degrees depending on the hour of the day 
      # and weekday/weekend
      "tc2"=c("coolingLpf2",
              "fs_components(...,featuresName='hour',
                             nHarmonics=ceiling(param$nhar/2),inplace=F)",
              "s"
      ),
      
      # Regression components for the heating intercept depending on the hour 
      # of the day and weekday/weekend
      "thint"=c("heatingLpfBool",
                "s"
      ),
      
      # Regression components for the cooling intercept depending on the hour 
      # of the day and weekday/weekend
      "tcint"=c("coolingLpfBool",
                "s"
      )
    )
```

# Data modelling 

Adjust a time-varying regression model - Recursive Least Squares (RLS) - to the data, considering the data transformation sentences, the weather dependence characteristics and the general parameters defined above.

Steps:

* Train the model and optimise the parameters using the validation techniques chosen in trControl.

* Create the predictor used by MLFlow containing the trained model and all its requirements.

* Compute interactive plots related with the trained model.

* Compute the errors related to the model.

* Load all the information to MLFlow.

```{r}
###
# Model training ----
###

write("* Training of the model and loading process to MLFlow",stderr())
trControl <- trainControl(method="none")    
suppressWarnings(
  with(mlflow_start_run(experiment_id = experimentId), {
    
    if(wdep$heating && wdep$cooling){
      write("   Model type: Heating and cooling",stderr())
      params <- generalParams[c("nhar","maxh","maxc","alpha","lambda")]
      transformationSentences <- generalTransformationSentences[
        c("s","intercept_s","daily_seasonality","weekly_seasonality","yearly_seasonality",
          "temperature","tlpf","temperatureBalanceHeating","temperatureBalanceCooling",
          "onOffHeating","onOffCooling","heatingLpf","coolingLpf",
          "heatingLpf2","coolingLpf2","heatingLpfBool","coolingLpfBool",
          "th","tc","th2","tc2","tcint","thint")]
      minMonthsTraining <- 9
    	HC_model <- function(params, df, ...) {
        args <- list(...)
        train(
          Qe ~ daily_seasonality + tc + th + tcint + thint,
          data=df,
          method = RLS(
            data.frame(parameter = names(params),
                       class = mapply(names(params),FUN=function(i) generalParams[[i]][["datatype"]]))
          ),
          tuneGrid = expand.grid(params), 
          trControl = trControl,
          maxPredictionValue = max(df$Qe,na.rm=T) * 1.1,
          logOutput = T,
          minMonthsTraining = minMonthsTraining,
          continuousTime = T,
          transformationSentences = args$transformationSentences,
          weatherDependenceByCluster = args$weatherDependenceByCluster,
          clusteringResults = args$clusteringResults
        )
      }
      best_params <- hyperparameters_tuning(
        opt_criteria = "minimise",
        opt_function = function(X, df, ...) {
          mod <- HC_model(X, df, ...)
          expected <- df$Qe
          obtained <- biggr::predict.train(
            object = mod, 
            newdata = df, 
            forceGlobalInputFeatures = NULL,
            modelMinMaxHorizonInHours = 1,
            modelWindow = NULL,
            modelSelection = NULL
          )
          RMSE(expected, obtained, na.rm = T)
        },
        features = params,
        maxiter = 3,
        popSize = 8,
        df = df[df$outliers==F & !is.na(df$outliers),],
        parallel = F,
        transformationSentences = transformationSentences,
        weatherDependenceByCluster = weatherDependenceByCluster,
        clusteringResults = clust
      )
      mod <- HC_model(best_params, df[df$outliers==F & !is.na(df$outliers),],
        transformationSentences = transformationSentences,
        weatherDependenceByCluster = weatherDependenceByCluster,
        clusteringResults = clust
      )
    } else if (wdep$heating){
      write("   Model type: Only heating",stderr())
      params <- generalParams[c("nhar","maxh","alpha","lambda")]#,"tbalh"
      transformationSentences <- generalTransformationSentences[
        c("s","intercept_s","daily_seasonality","weekly_seasonality","yearly_seasonality",
          "temperature","tlpf","temperatureBalanceHeating","onOffHeating",
          "heatingLpf","heatingLpf2",
          "heatingLpfBool","th","th2","thint")]
      minMonthsTraining <- 4
      H_model <- function(params, df, ...) {
        args <- list(...)
        train(
          Qe ~ daily_seasonality + th + thint,
          data = df,
          method = RLS(
            data.frame(parameter = names(params),
                       class = mapply(names(params),FUN=function(i) generalParams[[i]][["datatype"]]))
          ),
          tuneGrid = expand.grid(params), 
          trControl = trControl,
          maxPredictionValue = max(df$Qe,na.rm=T) * 1.1,
          logOutput = T,
          minMonthsTraining = minMonthsTraining,
          continuousTime = T,
          transformationSentences = args$transformationSentences,
          weatherDependenceByCluster = args$weatherDependenceByCluster,
          clusteringResults = args$clusteringResults
        )
      }
      best_params <- hyperparameters_tuning(
        opt_criteria = "minimise",
        opt_function = function(X, df, ...) {
          mod <- H_model(X, df, ...)
          expected <- df$Qe
          obtained <- biggr::predict.train(
            object = mod, 
            newdata = df, 
            forceGlobalInputFeatures = NULL,
            modelMinMaxHorizonInHours = 1,
            modelWindow = NULL,
            modelSelection = NULL
          )
          RMSE(expected, obtained, na.rm = T)
        },
        features = params,
        maxiter = 3,
        popSize = 8,
        parallel = F,
        df = df[df$outliers==F & !is.na(df$outliers),],
        transformationSentences = transformationSentences,
        weatherDependenceByCluster = weatherDependenceByCluster,
        clusteringResults = clust
      )
      mod <- H_model(best_params, df[df$outliers==F & !is.na(df$outliers),],
        transformationSentences = transformationSentences,
        weatherDependenceByCluster = weatherDependenceByCluster,
        clusteringResults = clust
      )
    } else if (wdep$cooling){
      write("   Model type: Only Cooling",stderr())
      params <- generalParams[c("nhar","maxc","alpha","lambda")]#,"tbalc"
      transformationSentences <- generalTransformationSentences[
        c("s","intercept_s","daily_seasonality","weekly_seasonality","yearly_seasonality",
          "temperature","tlpf","temperatureBalanceCooling","onOffCooling",
          "coolingLpf","coolingLpf2",
          "coolingLpfBool","tc","tc2","tcint")]
      minMonthsTraining <- 4
      C_model <- function(params, df, ...) {
        args <- list(...)
        train(
          Qe ~ daily_seasonality + tc + tcint,
          data = df,
          method = RLS(
            data.frame(parameter = names(params),
                       class = mapply(names(params),FUN=function(i) generalParams[[i]][["datatype"]]))
          ),
          tuneGrid = expand.grid(params), 
          trControl = trControl,
          maxPredictionValue = max(df$Qe,na.rm=T) * 1.1,
          logOutput = T,
          minMonthsTraining = minMonthsTraining,
          continuousTime = T,
          transformationSentences = args$transformationSentences,
          weatherDependenceByCluster = args$weatherDependenceByCluster,
          clusteringResults = args$clusteringResults
        )
      }
      best_params <- hyperparameters_tuning(
        opt_criteria = "minimise",
        opt_function = function(X, df, ...) {
          mod <- C_model(X, df, ...)
          expected <- df$Qe
          obtained <- biggr::predict.train(
            object = mod, 
            newdata = df, 
            forceGlobalInputFeatures = NULL,
            modelMinMaxHorizonInHours = 1,
            modelWindow = NULL,
            modelSelection = NULL
          )
          RMSE(expected, obtained, na.rm = T)
        },
        features = params,
        maxiter = 3,
        popSize = 8,
        parallel = F,
        df = df[df$outliers==F & !is.na(df$outliers),],
        transformationSentences = transformationSentences,
        weatherDependenceByCluster = weatherDependenceByCluster,
        clusteringResults = clust
      )
      mod <- C_model(best_params, df[df$outliers==F & !is.na(df$outliers),],
        transformationSentences = transformationSentences,
        weatherDependenceByCluster = weatherDependenceByCluster,
        clusteringResults = clust
      )
    } else {
      write("   Model type: Not weather dependence",stderr())
      params <- generalParams[c("nhar","lambda")]
      transformationSentences <- generalTransformationSentences[
        c("s","intercept_s","daily_seasonality","weekly_seasonality","yearly_seasonality")
      ]
      minMonthsTraining <- 1
      CAL_model <- function(params, df, ...) {
        args <- list(...)
        train(
          Qe ~ intercept_s + daily_seasonality,
          data=df,
          method = RLS(
            data.frame(parameter = names(params),
                       class = mapply(names(params),FUN=function(i) generalParams[[i]][["datatype"]]))
          ),
          tuneGrid = expand.grid(params), 
          trControl = trControl,
          maxPredictionValue = max(df[df$outliers==F & !is.na(df$outliers),"Qe"],na.rm=T) * 1.1,
          logOutput = T,
          minMonthsTraining = minMonthsTraining,
          continuousTime = T,
          transformationSentences = args$transformationSentences,
          clusteringResults = args$clusteringResults
        )
      }
      best_params <- hyperparameters_tuning(
        opt_criteria = "minimise",
        opt_function = function(X, df, ...) {
          mod <- CAL_model(X, df, ...)
          expected <- df$Qe
          obtained <- biggr::predict.train(
            object = mod, 
            newdata = df, 
            forceGlobalInputFeatures = NULL,
            modelMinMaxHorizonInHours = 1,
            modelWindow = NULL,
            modelSelection = NULL
          )
          RMSE(expected, obtained, na.rm = T)
        },
        features = params,
        maxiter = 3,
        popSize = 8,
        parallel = F,
        df = df[df$outliers==F & !is.na(df$outliers),],
        transformationSentences = transformationSentences,
        clusteringResults = clust
      )
      mod <- CAL_model(best_params, df[df$outliers==F & !is.na(df$outliers),],
        transformationSentences = transformationSentences,
        clusteringResults = clust
      )
    }
    
    # Generate the predictor object
    predictor <- crate(function(x, forceGlobalInputFeatures = NULL,modelMinMaxHorizonInHours=1,
                                modelWindow="%Y-%m-%d", modelSelection="rmse"){
      biggr::predict.train(
        object = !!mod, 
        newdata = x, 
        forceGlobalInputFeatures = forceGlobalInputFeatures,
        modelMinMaxHorizonInHours = modelMinMaxHorizonInHours,
        modelWindow = modelWindow,
        modelSelection = modelSelection
      )
    })
  
    df_result <- df[df$time >= (min(df$time)+months(minMonthsTraining)),]
    df_result$predicted <- predictor(df_result)
    
    # Actual vs. predicted
    p <- ggplotly(
      ggplot(df_result) + 
        geom_line(aes(time,Qe), col="black") + 
        geom_line(aes(time,predicted),col="red",alpha=0.5))
    saveWidget(p, "series_comparison.html", 
               selfcontained = T)
    
    # Residuals plot
    p <- ggplotly(
      ggplot(df_result) + 
        geom_line(aes(time,Qe-predicted), col="black"))
    saveWidget(p, "residuals.html", 
               selfcontained = T)
    
    # Generate the MLflow instance
    mlflow_log_param("Model",modelName)
    
    # Store the params
    for (i in 1:ncol(mod$bestTune)){
      mlflow_log_param(colnames(mod$bestTune)[i],as.character(mod$bestTune[1,i]))
    }
    # Store the errors
    mlflow_log_metric("MAE", 
                      MAE(df_result$Qe[df_result$outliers==F],
                          df_result$predicted[df_result$outliers==F],na.rm = T))
    mlflow_log_metric("RMSE", RMSE(df_result$Qe[df_result$outliers==F],
                                   df_result$predicted[df_result$outliers==F],na.rm = T))
    mlflow_log_metric("CVRMSE", 
                      RMSE(df_result$Qe[df_result$outliers==F],
                           df_result$predicted[df_result$outliers==F],na.rm = T)/
                        mean(df_result$Qe[df_result$outliers==F],na.rm=T))
    mlflow_log_metric("r2", cor(df_result$Qe[df_result$outliers==F],
                                df_result$predicted[df_result$outliers==F],
                                use = "na.or.complete")^2)
    # Store the artifacts
    mlflow_log_artifact("raw_consumption_temperature.html")
    mlflow_log_artifact("series_comparison.html")
    mlflow_log_artifact("residuals.html")
    mlflow_log_artifact("outliers_plot.html")
    mlflow_log_artifact("clustering_signature.html")
    mlflow_log_artifact("clustering_dlc.html")
    mlflow_log_artifact("clustering_dlc_ts.html")
    mlflow_log_artifact("clustering_dlc_wdep.pdf")
    
    # Register the model
    tryCatch(mlflow_create_registered_model(paste0(identifier,"~",modelName)),error=function(e){})
    modelId <- mlflow_get_run()$run_uuid[1]
    modelSubject <- sprintf("runs:/%s/model",modelId)
    registeredModelName <- paste0(identifier,"~",modelName)
    mlflow_create_model_version(name = registeredModelName,
                                run_link = modelSubject)
    
    # Store the model
    mlflow_log_model(predictor,"model")
    mlflow_log_param("modelId",modelId)
    mlflow_log_param("modelSubject",modelSubject)
    mlflow_log_param("registeredModelName",registeredModelName)
    
  })
)
```

[Results in MLFlow](http://127.0.0.1:5000/)

# Results for longitudinal benchmarking

TO DO Eloi!!!

## Non-intrusive consumption dissagregation

Disaggregation of the total electricity consumption into:

* Heating component

* Cooling component 

* Baseload component

Based on the trained model and the real consumption time series. 

Afterwards, the components can be aggregated to greater timesteps (daily, weekly, monthly, or yearly) and used to calculate energy use intensity indicators, such as kWh per area, to better compare against other buildings in the case of cross-sectional benchmarking.

```{r}
# Load the "rls" registered model
predictor_loaded<-mlflow_load_model(
  mlflow_get_registered_model(paste0(buildingId,"~rls"))$latest_versions[[1]]$run_link
  #mlflow_get_latest_versions(paste0(buildingId,"~rls"))[[1]]$run_link
)

# Forcing only heating and only cooling dependency
baseload_and_cooling <- predictor_loaded( df_for_pred, 
  forceGlobalInputFeatures = list("heatingLpf2"=0, "heatingLpf"=0, "heatingLpfBool"=0) )
baseload_and_heating <- predictor_loaded( df_for_pred, 
  forceGlobalInputFeatures = list("coolingLpf2"=0, "coolingLpf"=0, "coolingLpfBool"=0) )

# Estimate the baseload consumption along the period
baseload <- predictor_loaded( df_for_pred, 
  forceGlobalInputFeatures = list("heatingLpf2"=0,"heatingLpf"=0, "heatingLpfBool"=0, 
                                  "coolingLpf2"=0,"coolingLpf"=0, "coolingLpfBool"=0) )

#plot(baseload_and_cooling,type="l");lines(baseload_and_heating,col="red");lines(baseload,col="green")

# Disaggregated predicted components and actual consumption
disaggregated_df <- data.frame(
      "time"=df_for_pred$time,
      "temperature"=df_for_pred$temperature,
      "real"=df_for_pred$Qe,
      "baseload"=baseload,
      "heating"=baseload_and_heating-baseload, 
      "cooling"=baseload_and_cooling-baseload
)
disaggregated_df$heatingSmooth <- rollmean(ifelse(disaggregated_df$heating>0,
                                                  disaggregated_df$heating,0), k = 3,
                                           align = "center", partial = T, fill = c(0,0,0))
disaggregated_df$coolingSmooth <- rollmean(ifelse(disaggregated_df$cooling>0,
                                                  disaggregated_df$cooling,0), k = 3,
                                           align = "center", partial = T, fill = c(0,0,0))
disaggregated_df$baseloadR <- ifelse(disaggregated_df$baseload > disaggregated_df$real,
                                     disaggregated_df$real, disaggregated_df$baseload)
disaggregated_df$heatingR <- ifelse(
  disaggregated_df$heatingSmooth > 0.1,
  ifelse(
    disaggregated_df$heatingSmooth > 0.1 & disaggregated_df$coolingSmooth > 0.1,
    # Heating and cooling at the same time
    (disaggregated_df$real - disaggregated_df$baseloadR) *
      (disaggregated_df$heatingSmooth/
         (disaggregated_df$heatingSmooth+disaggregated_df$coolingSmooth)),
    # Only heating
    disaggregated_df$real - disaggregated_df$baseloadR),
  0
)
disaggregated_df$coolingR <- ifelse(
  disaggregated_df$coolingSmooth > 0.1,
  disaggregated_df$real - (disaggregated_df$baseloadR + disaggregated_df$heatingR),
  0)
disaggregated_df$baseloadR <- disaggregated_df$real - (disaggregated_df$coolingR + disaggregated_df$heatingR)

fig <- plot_ly(disaggregated_df, x = ~time, y = ~baseloadR, name = 'Baseload', type = 'scatter', mode = 'none', stackgroup = 'one', fillcolor = '#BFBBBB', yaxis = list(title = 'Qe'))
fig <- fig %>% add_trace(y = ~heatingR, name = 'Heating', stackgroup = 'one', fillcolor = '#CF3737')
fig <- fig %>% add_trace(y = ~coolingR, name = 'Cooling', stackgroup = 'one', fillcolor = '#1D8DEB')
fig

```

If these results are summarised to daily data, the disaggregation components become more clear over a large period.

```{r}
disaggregated_daily_df <- aggregate(
  disaggregated_df[,c("baseload","heating","cooling","baseloadR",
                      "heatingR","coolingR")],
  by=list("time"=as.Date(disaggregated_df$time, tz="Europe/Madrid")),
  FUN=sum)
fig <- plot_ly(disaggregated_daily_df, x = ~time, y = ~baseloadR, name = 'Baseload', type = 'scatter', mode = 'none', stackgroup = 'one', fillcolor = '#BFBBBB', yaxis = list(title = 'Qe'))
fig <- fig %>% add_trace(y = ~heatingR, name = 'Heating', fillcolor = '#CF3737')
fig <- fig %>% add_trace(y = ~coolingR, name = 'Cooling', fillcolor = '#1D8DEB')
fig
```

## One-year horizon backcasting

TODO: Include a backcasting diagram and an intro to this section

```{r}
pred_distribution <- 
  predictor_loaded( df_for_pred, 
                    modelMinMaxHorizonInHours=
                      # 10 to 14 month of backcasting
                      if(sum(df_for_pred$time > 
                             (min(mod$finalModel$localtime) +
                              hours(mod$finalModel$meta$minMonthsTraining*30*24))
                          ) >= hourly_timesteps(14*30*24,detect_time_step(df_for_pred$time))
                      ){
                        c(31*11*24,31*13*24)
                      } else if(sum(df_for_pred$time > 
                             (min(mod$finalModel$localtime) +
                              hours(mod$finalModel$meta$minMonthsTraining*30*24))
                          ) < hourly_timesteps(365*24,detect_time_step(df_for_pred$time))
                      ){
                        stop("No sufficient period to make a backcasting, at least 1 year more than the minMonthTraining parameter of the model")
                      # 0 to n months of backcasting
                      } else{
                        c(hourly_timesteps(0*24,detect_time_step(df_for_pred$time)),
                          hourly_timesteps(sum(df_for_pred$time > 
                             (min(mod$finalModel$localtime) +
                              hours(mod$finalModel$meta$minMonthsTraining*30*24))
                          ),detect_time_step(df_for_pred$time)))
                      },
                    modelWindow="%Y-%W", 
                    modelSelection="rmse" ) %>%
  left_join(df_for_pred[,c("localtime","Qe")],"localtime")

pred_distribution <- pred_distribution[complete.cases(pred_distribution),]

ggplotly(ggplot(pred_distribution) + geom_line(aes(localtime,Qe)) +
  geom_line(aes(localtime,pred),col="red", alpha = 0.5) +
  geom_ribbon(aes(localtime,ymin=`10.0%`,ymax=`90.0%`),color="blue", alpha=0.3))
```

Evolution of the energy saving over time.

```{r}
vars_to_resample <- colnames(pred_distribution)[!colnames(pred_distribution) %in% "localtime"]
pred_distribution_d <- aggregate(
  pred_distribution[,vars_to_resample],
  by=list("date"=as.Date(pred_distribution$localtime)),
  FUN=sum)
pred_distribution_d <- cbind(
  setNames(
    as.data.frame(mapply(function(v){
      rollmean(pred_distribution_d[,v],k = 31,fill = c(NA,NA,NA),align="center",partial=T)
    }, vars_to_resample)),
  paste0(vars_to_resample,"_smoothed")
  ),
  pred_distribution_d)
ggplotly(ggplot(pred_distribution_d[complete.cases(pred_distribution_d),]) + 
           geom_hline(aes(yintercept=0), linetype="dashed") +
           geom_ribbon( aes(date, ymin=0,
                            ymax=ifelse((`Qe_smoothed`-`mean_smoothed`)<0,
                                        (`Qe_smoothed`-`mean_smoothed`)/
                                          `mean_smoothed`,0)), 
                        fill="green", alpha=0.3 ) +
           geom_ribbon( aes(date, ymax=0,
                            ymin=ifelse((`Qe_smoothed`-`mean_smoothed`)>0,
                                        (`Qe_smoothed`-`mean_smoothed`)/
                                          `mean_smoothed`,0)), 
                        fill="red", alpha=0.3 ) +
           geom_ribbon( aes(date, ymin=(`Qe_smoothed`-`75.0%_smoothed`)/`75.0%_smoothed`,
                            ymax=(`Qe_smoothed`-`25.0%_smoothed`)/`25.0%_smoothed`), 
                        fill="grey", alpha=0.5 ) +
           geom_ribbon( aes(date, ymin=(`Qe_smoothed`-`97.5%_smoothed`)/`97.5%_smoothed`,
                            ymax=(`Qe_smoothed`-`2.5%_smoothed`)/`2.5%_smoothed`), 
                        fill="grey", alpha=0.2 ) +
           geom_ribbon( aes(date, ymin=(`Qe_smoothed`-`90.0%_smoothed`)/`90.0%_smoothed`,
                            ymax=(`Qe_smoothed`-`10.0%_smoothed`)/`10.0%_smoothed`), 
                        fill="grey", alpha=0.3 ) +
           geom_line(aes(date,(`Qe_smoothed`-`mean_smoothed`)/`mean_smoothed`),
                     col="blue", alpha=0.8, size = 0.5) +
           geom_line(aes(date,(`Qe_smoothed`-`pred_smoothed`)/`pred_smoothed`),
                     col="darkred", alpha=0.8, size = 1) +
           theme_bw() + ylab("Daily energy savings (%)") + scale_y_continuous(labels=scales::percent)
        )
ggplotly(ggplot(pred_distribution_d[complete.cases(pred_distribution_d),]) + 
           geom_hline(aes(yintercept=0), linetype="dashed") +
           geom_line(aes(date,Qe_smoothed),alpha=0.2)+
           geom_line(aes(date,mean_smoothed),alpha=0.2,col="blue")+
           geom_line(aes(date,pred_smoothed),alpha=0.2,col="red")+
           geom_ribbon( aes(date, ymin=0,
                            ymax=ifelse((`Qe_smoothed`-`mean_smoothed`)<0,
                                        (`Qe_smoothed`-`mean_smoothed`),0)), 
                        fill="green", alpha=0.3 ) +
           geom_ribbon( aes(date, ymax=0,
                            ymin=ifelse((`Qe_smoothed`-`mean_smoothed`)>0,
                                        (`Qe_smoothed`-`mean_smoothed`),0)), 
                        fill="darkred", alpha=0.3 ) +
           geom_ribbon( aes(date, ymin=(`Qe_smoothed`-`75.0%_smoothed`),
                            ymax=(`Qe_smoothed`-`25.0%_smoothed`)), 
                        fill="grey", alpha=0.5 ) +
           geom_ribbon( aes(date, ymin=(`Qe_smoothed`-`97.5%_smoothed`),
                            ymax=(`Qe_smoothed`-`2.5%_smoothed`)), 
                        fill="grey", alpha=0.2 ) +
           geom_ribbon( aes(date, ymin=(`Qe_smoothed`-`90.0%_smoothed`),
                            ymax=(`Qe_smoothed`-`10.0%_smoothed`)), 
                        fill="grey", alpha=0.3 ) +
           geom_line(aes(date,(`Qe_smoothed`-`mean_smoothed`)),
                     col="blue", alpha=0.8, size = 0.5) +
           geom_line(aes(date,(`Qe_smoothed`-`pred_smoothed`)),
                     col="darkred", alpha=0.8, size = 1) +
           theme_bw() + ylab("Daily energy differentials (kWh)")
        )
```
